{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FewShot_maml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vainaijr/few-shot-maml/blob/master/FewShot_maml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqXXH4c7Zs8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch-1.0.0a0+1d94a2b-cp36-cp36m-linux_x86_64.whl  \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch_xla-0.1+5622d42-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPJK6ArUOgUj",
        "colab_type": "code",
        "outputId": "6149f2ff-d78f-4432-f93c-08c2ab2054fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "%%shell\n",
        "pip install --upgrade tb-nightly"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tb-nightly in /usr/local/lib/python3.6/dist-packages (1.14.0a20190604)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly) (3.7.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nfP6bpmZs6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch_xla\n",
        "import torch_xla\n",
        "import torch_xla_py.utils as xu\n",
        "import torch_xla_py.xla_model as xm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P6yKpP_IS0F",
        "colab_type": "code",
        "outputId": "da8fc91c-d1f2-424f-8118-8473f0670892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0404q9CR-uc",
        "colab_type": "code",
        "outputId": "08980adb-c98f-439e-c47e-f10c1a7f3fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-05 09:22:44--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.73.9.93, 34.206.130.40, 52.203.66.95, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.73.9.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  34.5MB/s    in 0.5s    \n",
            "\n",
            "2019-06-05 09:22:45 (34.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dbpe-wR-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '/gdrive/My\\ Drive/runs/'\n",
        "get_ipython().system_raw(\n",
        "    \"tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\".format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihFl1jdrR-sh",
        "colab_type": "code",
        "outputId": "13dd29dd-9c3d-44b7-974d-dda50a899d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://56f635de.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbi7rvJISrRd",
        "colab_type": "code",
        "outputId": "a6a7d4d4-b0f4-4fbc-994a-5c7b8bda1300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch import cat, topk, bmm, stack, norm, zeros, ones, sum, transpose, save, load, manual_seed, cuda\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from torch.nn import init\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Sequential\n",
        "from torch.nn import Upsample\n",
        "from torch.nn import ReflectionPad2d, ZeroPad2d\n",
        "from torch.nn import Conv2d, ConvTranspose2d, Linear\n",
        "from torch.nn import BatchNorm1d, BatchNorm2d, InstanceNorm2d\n",
        "from torch.nn import PixelShuffle\n",
        "from torch.nn import Dropout, Dropout2d\n",
        "from torch.nn import LeakyReLU, ReLU, PReLU, Softmax, Tanh, Sigmoid\n",
        "from torch.nn import AdaptiveAvgPool2d, AdaptiveMaxPool2d, MaxPool2d, AvgPool2d\n",
        "from torch.nn import RNN, LSTM\n",
        "from torch.nn import MSELoss, L1Loss, BCELoss, NLLLoss, BCEWithLogitsLoss, CrossEntropyLoss\n",
        "\n",
        "from torch.nn.functional import relu6, avg_pool2d, softmax, interpolate, linear, conv2d, batch_norm\n",
        "from torch.nn.functional import relu, sigmoid\n",
        "\n",
        "from torch.autograd import grad, Variable as V\n",
        "from torch.nn.utils import weight_norm \n",
        "from torch.optim import Adam, SGD, LBFGS\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.checkpoint import checkpoint as cp\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomResizedCrop, ToTensor, Normalize\n",
        "from torchvision.transforms import CenterCrop, Resize, ColorJitter, ToPILImage, RandomCrop, RandomSizedCrop\n",
        "from torchvision.models import ResNet, vgg19, vgg19_bn, densenet201, resnet101, resnet34, resnext101_32x8d\n",
        "from torchvision.models import resnet152, resnet18, vgg16 \n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.weight_norm import WeightNorm\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from abc import abstractmethod\n",
        "from PIL import ImageEnhance\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "import torch.optim\n",
        "import json\n",
        "import torch.utils.data.sampler\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import h5py\n",
        "import argparse\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "identity = lambda x:x\n",
        "step_ = 0\n",
        "step__ = 0\n",
        "stepp = 0\n",
        "writer = SummaryWriter(log_dir='/gdrive/My Drive/runs/')\n",
        "unloader = ToPILImage()\n",
        "save_dir                    = '/gdrive/My Drive/save_dir_CUB/'\n",
        "data_dir = {}\n",
        "data_dir['CUB']             = '/gdrive/My Drive/CUB/' \n",
        "data_dir['miniImagenet']    = './filelists/miniImagenet/' \n",
        "data_dir['omniglot']        = './filelists/omniglot/' \n",
        "data_dir['emnist']          = './filelists/emnist/' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA available:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8JEnfF-pPAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code is modified from https://github.com/facebookresearch/low-shot-shrink-hallucinate\n",
        "\n",
        "# Basic ResNet model\n",
        "\n",
        "def init_layer(L):\n",
        "    # print(\"init_layer\")\n",
        "    classname = L.__class__.__name__\n",
        "    # Initialization using fan-in\n",
        "    if isinstance(L, nn.Conv2d):\n",
        "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
        "        L.weight.data.normal_(0, math.sqrt(2.0/float(n)))\n",
        "    elif isinstance(L, nn.BatchNorm2d):\n",
        "        L.weight.data.fill_(1)\n",
        "        L.bias.data.fill_(0)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        init.normal_(L.weight.data, 0.0, 0.02)\n",
        "\n",
        "class distLinear(nn.Module):\n",
        "    def __init__(self, indim, outdim):\n",
        "        super(distLinear, self).__init__()\n",
        "        self.L = nn.Linear( indim, outdim, bias = False)\n",
        "        self.class_wise_learnable_norm = True  #See the issue#4&8 in the github \n",
        "        if self.class_wise_learnable_norm:      \n",
        "            WeightNorm.apply(self.L, 'weight', dim=0) #split the weight update component to direction and norm      \n",
        "\n",
        "        if outdim <=200:\n",
        "            self.scale_factor = 2; #a fixed scale factor to scale the output of cos value into a reasonably large input for softmax\n",
        "        else:\n",
        "            self.scale_factor = 10; #in omniglot, a larger scale factor is required to handle >1000 output classes.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = torch.norm(x, p=2, dim =1).unsqueeze(1).expand_as(x)\n",
        "        x_normalized = x.div(x_norm+ 0.00001)\n",
        "        if not self.class_wise_learnable_norm:\n",
        "            L_norm = torch.norm(self.L.weight.data, p=2, dim =1).unsqueeze(1).expand_as(self.L.weight.data)\n",
        "            self.L.weight.data = self.L.weight.data.div(L_norm + 0.00001)\n",
        "        cos_dist = self.L(x_normalized) #matrix product by forward function, but when using WeightNorm, this also multiply the cosine distance by a class-wise learnable norm, see the issue#4&8 in the github\n",
        "        scores = self.scale_factor* (cos_dist) \n",
        "\n",
        "        return scores\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Linear_fw(nn.Linear): #used in MAML to forward input with fast weight \n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear_fw, self).__init__(in_features, out_features)\n",
        "        self.weight.fast = None #Lazy hack to add fast weight link\n",
        "        self.bias.fast = None\n",
        "        print(\"__init__Linear_fw\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.weight.fast is not None and self.bias.fast is not None:\n",
        "            out = F.linear(x, self.weight.fast, self.bias.fast) #weight.fast (fast weight) is the temporaily adapted weight\n",
        "            print(\"forward_Linear_fw_fast\")\n",
        "        else:\n",
        "            print(\"forward_Linear_fw_1\")\n",
        "#             grid_ = make_grid(x)\n",
        "            print(\"x.size() {}\".format(x.size()))\n",
        "            # writer.add_image('out_Linear_fw', x)\n",
        "            out = super(Linear_fw, self).forward(x)\n",
        "            print(\"out.size() {}\".format(out.size()))\n",
        "            print(\"forward_Linear_fw_2\")\n",
        "            \n",
        "            \n",
        "        return out\n",
        "\n",
        "class Conv2d_fw(nn.Conv2d): #used in MAML to forward input with fast weight \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,padding=0, bias = True):\n",
        "        super(Conv2d_fw, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.weight.fast = None\n",
        "        if not self.bias is None:\n",
        "            self.bias.fast = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Conv2d_fw_x.size() {}\".format(x.size()))\n",
        "#         print(\"self.bias {}\".format(self.bias))\n",
        "#         print(\"self.weight.fast {}\".format(self.weight.fast))\n",
        "#         print(\"self.bias.fast {}\".format(self.bias.fast))\n",
        "        if self.bias is None:\n",
        "            if self.weight.fast is not None:\n",
        "                out = F.conv2d(x, self.weight.fast, None, stride= self.stride, padding=self.padding)\n",
        "            else:\n",
        "                out = super(Conv2d_fw, self).forward(x)\n",
        "        else:\n",
        "            if self.weight.fast is not None and self.bias.fast is not None:\n",
        "                out = F.conv2d(x, self.weight.fast, self.bias.fast, stride= self.stride, padding=self.padding)\n",
        "            else:\n",
        "                out = super(Conv2d_fw, self).forward(x)\n",
        "        \n",
        "        T = out\n",
        "        ER = T\n",
        "        # print(\"T.unsqueeze {}\".format(ER.unsqueeze(2).size()))\n",
        "        grid___ = make_grid(T.view(out.size(0) * out.size(1), 1, out.size(2), out.size(3)))\n",
        "        # print(\"out size {}\".format(out.size()))\n",
        "        # print(\"grid size {}\".format(grid___.size()))\n",
        "        \n",
        "        writer.add_image(\"Conv2d_fw_1\", grid___, 0, dataformats='CHW')\n",
        "        \n",
        "        \n",
        "        # print(\"Conv2d_fw_out.size() {}\".format(out.size()))\n",
        "        return out\n",
        "            \n",
        "class BatchNorm2d_fw(nn.BatchNorm2d): #used in MAML to forward input with fast weight \n",
        "    def __init__(self, num_features):\n",
        "        super(BatchNorm2d_fw, self).__init__(num_features)\n",
        "        self.weight.fast = None\n",
        "        self.bias.fast = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"BatchNorm2d_fw_x.size() {}\".format(x.size()))\n",
        "        running_mean = torch.zeros(x.data.size()[1]).cuda()\n",
        "        running_var = torch.ones(x.data.size()[1]).cuda()\n",
        "        if self.weight.fast is not None and self.bias.fast is not None:\n",
        "            out = F.batch_norm(x, running_mean, running_var, self.weight.fast, self.bias.fast, training = True, momentum = 1)\n",
        "            #batch_norm momentum hack: follow hack of Kate Rakelly in pytorch-maml/src/layers.py\n",
        "        else:\n",
        "            out = F.batch_norm(x, running_mean, running_var, self.weight, self.bias, training = True, momentum = 1)\n",
        "        # print(\"BatchNorm2d_fw_out.size() {}\".format(out.size()))\n",
        "        return out\n",
        "\n",
        "# Simple Conv Block\n",
        "class ConvBlock(nn.Module):\n",
        "    maml = False #Default\n",
        "    def __init__(self, indim, outdim, pool = True, padding = 1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.indim  = indim\n",
        "        self.outdim = outdim\n",
        "        if self.maml:\n",
        "            self.C      = Conv2d_fw(indim, outdim, 3, padding = padding)\n",
        "            self.BN     = BatchNorm2d_fw(outdim)\n",
        "        else:\n",
        "            self.C      = nn.Conv2d(indim, outdim, 3, padding= padding)\n",
        "            self.BN     = nn.BatchNorm2d(outdim)\n",
        "        self.relu   = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.parametrized_layers = [self.C, self.BN, self.relu]\n",
        "        if pool:\n",
        "            self.pool   = nn.MaxPool2d(2)\n",
        "            self.parametrized_layers.append(self.pool)\n",
        "\n",
        "        for layer in self.parametrized_layers:\n",
        "            init_layer(layer)\n",
        "\n",
        "        self.trunk = nn.Sequential(*self.parametrized_layers)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.trunk(x)\n",
        "#         grid = make_grid(out)\n",
        "#         writer.add_image('ConvBlock', grid)\n",
        "        return out\n",
        "\n",
        "# # Simple ResNet Block\n",
        "class SimpleBlock(nn.Module):\n",
        "    maml = False #Default\n",
        "    def __init__(self, indim, outdim, half_res):\n",
        "        super(SimpleBlock, self).__init__()\n",
        "        self.indim = indim\n",
        "        self.outdim = outdim\n",
        "        if self.maml:\n",
        "            self.C1 = Conv2d_fw(indim, outdim, kernel_size=3, stride=2 if half_res else 1, padding=1, bias=False)\n",
        "            self.BN1 = BatchNorm2d_fw(outdim)\n",
        "            self.C2 = Conv2d_fw(outdim, outdim,kernel_size=3, padding=1,bias=False)\n",
        "            self.BN2 = BatchNorm2d_fw(outdim)\n",
        "        else:\n",
        "            self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if half_res else 1, padding=1, bias=False)\n",
        "            self.BN1 = nn.BatchNorm2d(outdim)\n",
        "            self.C2 = nn.Conv2d(outdim, outdim,kernel_size=3, padding=1,bias=False)\n",
        "            self.BN2 = nn.BatchNorm2d(outdim)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n",
        "\n",
        "        self.half_res = half_res\n",
        "\n",
        "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
        "        if indim!=outdim:\n",
        "            if self.maml:\n",
        "                self.shortcut = Conv2d_fw(indim, outdim, 1, 2 if half_res else 1, bias=False)\n",
        "                self.BNshortcut = BatchNorm2d_fw(outdim)\n",
        "            else:\n",
        "                self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 1, bias=False)\n",
        "                self.BNshortcut = nn.BatchNorm2d(outdim)\n",
        "\n",
        "            self.parametrized_layers.append(self.shortcut)\n",
        "            self.parametrized_layers.append(self.BNshortcut)\n",
        "            self.shortcut_type = '1x1'\n",
        "        else:\n",
        "            self.shortcut_type = 'identity'\n",
        "\n",
        "        for layer in self.parametrized_layers:\n",
        "            init_layer(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.C1(x)\n",
        "        out = self.BN1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.C2(out)\n",
        "        out = self.BN2(out)\n",
        "        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(self.shortcut(x))\n",
        "        out = out + short_out\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# # Bottleneck block\n",
        "# class BottleneckBlock(nn.Module):\n",
        "#     maml = False #Default\n",
        "#     def __init__(self, indim, outdim, half_res):\n",
        "#         super(BottleneckBlock, self).__init__()\n",
        "#         bottleneckdim = int(outdim/4)\n",
        "#         self.indim = indim\n",
        "#         self.outdim = outdim\n",
        "#         if self.maml:\n",
        "#             self.C1 = Conv2d_fw(indim, bottleneckdim, kernel_size=1,  bias=False)\n",
        "#             self.BN1 = BatchNorm2d_fw(bottleneckdim)\n",
        "#             self.C2 = Conv2d_fw(bottleneckdim, bottleneckdim, kernel_size=3, stride=2 if half_res else 1,padding=1)\n",
        "#             self.BN2 = BatchNorm2d_fw(bottleneckdim)\n",
        "#             self.C3 = Conv2d_fw(bottleneckdim, outdim, kernel_size=1, bias=False)\n",
        "#             self.BN3 = BatchNorm2d_fw(outdim)\n",
        "#         else:\n",
        "#             self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1,  bias=False)\n",
        "#             self.BN1 = nn.BatchNorm2d(bottleneckdim)\n",
        "#             self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3, stride=2 if half_res else 1,padding=1)\n",
        "#             self.BN2 = nn.BatchNorm2d(bottleneckdim)\n",
        "#             self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n",
        "#             self.BN3 = nn.BatchNorm2d(outdim)\n",
        "\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2, self.C3, self.BN3]\n",
        "#         self.half_res = half_res\n",
        "\n",
        "\n",
        "#         # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
        "#         if indim!=outdim:\n",
        "#             if self.maml:\n",
        "#                 self.shortcut = Conv2d_fw(indim, outdim, 1, stride=2 if half_res else 1, bias=False)\n",
        "#             else:\n",
        "#                 self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if half_res else 1, bias=False)\n",
        "\n",
        "#             self.parametrized_layers.append(self.shortcut)\n",
        "#             self.shortcut_type = '1x1'\n",
        "#         else:\n",
        "#             self.shortcut_type = 'identity'\n",
        "\n",
        "#         for layer in self.parametrized_layers:\n",
        "#             init_layer(layer)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)\n",
        "#         out = self.C1(x)\n",
        "#         out = self.BN1(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.C2(out)\n",
        "#         out = self.BN2(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.C3(out)\n",
        "#         out = self.BN3(out)\n",
        "#         out = out + short_out\n",
        "\n",
        "#         out = self.relu(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, depth, flatten = True):\n",
        "        super(ConvNet,self).__init__()\n",
        "        trunk = []\n",
        "        for i in range(depth):\n",
        "            indim = 3 if i == 0 else 64\n",
        "            outdim = 64\n",
        "            B = ConvBlock(indim, outdim, pool = ( i <4 ) ) #only pooling for fist 4 layers\n",
        "            trunk.append(B)\n",
        "\n",
        "        if flatten:\n",
        "            trunk.append(Flatten())\n",
        "        # print(\"ConvNet_init\")\n",
        "        self.trunk = nn.Sequential(*trunk)\n",
        "        self.final_feat_dim = 1600\n",
        "      \n",
        "    def forward(self,x):\n",
        "        # print(\"ConvNet_forward\")\n",
        "        # print('x size {}'.format(x.size()))\n",
        "        \n",
        "        \n",
        "        grid = make_grid(x)\n",
        "        writer.add_image('x_', grid)\n",
        "        out = self.trunk(x)\n",
        "        # print('out size {}'.format(out.size()))\n",
        "        \n",
        "#         plt.figure(figsize=(8,8))\n",
        "#         plt.axis(\"off\")\n",
        "#         plt.imshow(out.data.cpu().numpy())\n",
        "#         plt.show()\n",
        "\n",
        "#         grid = make_grid(out)\n",
        "#         writer.add_image('ConvNet', grid)\n",
        "        return out\n",
        "\n",
        "# class ConvNetNopool(nn.Module): #Relation net use a 4 layer conv with pooling in only first two layers, else no pooling\n",
        "#     def __init__(self, depth):\n",
        "#         super(ConvNetNopool,self).__init__()\n",
        "#         trunk = []\n",
        "#         for i in range(depth):\n",
        "#             indim = 3 if i == 0 else 64\n",
        "#             outdim = 64\n",
        "#             B = ConvBlock(indim, outdim, pool = ( i in [0,1] ), padding = 0 if i in[0,1] else 1  ) #only first two layer has pooling and no padding\n",
        "#             trunk.append(B)\n",
        "\n",
        "#         self.trunk = nn.Sequential(*trunk)\n",
        "#         self.final_feat_dim = [64,19,19]\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = self.trunk(x)\n",
        "#         return out\n",
        "\n",
        "# class ConvNetS(nn.Module): #For omniglot, only 1 input channel, output dim is 64\n",
        "#     def __init__(self, depth, flatten = True):\n",
        "#         super(ConvNetS,self).__init__()\n",
        "#         trunk = []\n",
        "#         for i in range(depth):\n",
        "#             indim = 1 if i == 0 else 64\n",
        "#             outdim = 64\n",
        "#             B = ConvBlock(indim, outdim, pool = ( i <4 ) ) #only pooling for fist 4 layers\n",
        "#             trunk.append(B)\n",
        "\n",
        "#         if flatten:\n",
        "#             trunk.append(Flatten())\n",
        "\n",
        "#         self.trunk = nn.Sequential(*trunk)\n",
        "#         self.final_feat_dim = 64\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = x[:,0:1,:,:] #only use the first dimension\n",
        "#         out = self.trunk(out)\n",
        "#         return out\n",
        "\n",
        "# class ConvNetSNopool(nn.Module): #Relation net use a 4 layer conv with pooling in only first two layers, else no pooling. For omniglot, only 1 input channel, output dim is [64,5,5]\n",
        "#     def __init__(self, depth):\n",
        "#         super(ConvNetSNopool,self).__init__()\n",
        "#         trunk = []\n",
        "#         for i in range(depth):\n",
        "#             indim = 1 if i == 0 else 64\n",
        "#             outdim = 64\n",
        "#             B = ConvBlock(indim, outdim, pool = ( i in [0,1] ), padding = 0 if i in[0,1] else 1  ) #only first two layer has pooling and no padding\n",
        "#             trunk.append(B)\n",
        "\n",
        "#         self.trunk = nn.Sequential(*trunk)\n",
        "#         self.final_feat_dim = [64,5,5]\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = x[:,0:1,:,:] #only use the first dimension\n",
        "#         out = self.trunk(out)\n",
        "#         return out\n",
        "\n",
        "# class ResNet(nn.Module):\n",
        "#     maml = False #Default\n",
        "#     def __init__(self,block,list_of_num_layers, list_of_out_dims, flatten = True):\n",
        "#         # list_of_num_layers specifies number of layers in each stage\n",
        "#         # list_of_out_dims specifies number of output channel for each stage\n",
        "#         super(ResNet,self).__init__()\n",
        "#         assert len(list_of_num_layers)==4, 'Can have only four stages'\n",
        "#         if self.maml:\n",
        "#             conv1 = Conv2d_fw(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "#                                                bias=False)\n",
        "#             bn1 = BatchNorm2d_fw(64)\n",
        "#         else:\n",
        "#             conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "#                                                bias=False)\n",
        "#             bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "#         relu = nn.ReLU()\n",
        "#         pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "#         init_layer(conv1)\n",
        "#         init_layer(bn1)\n",
        "\n",
        "\n",
        "#         trunk = [conv1, bn1, relu, pool1]\n",
        "\n",
        "#         indim = 64\n",
        "#         for i in range(4):\n",
        "\n",
        "#             for j in range(list_of_num_layers[i]):\n",
        "#                 half_res = (i>=1) and (j==0)\n",
        "#                 B = block(indim, list_of_out_dims[i], half_res)\n",
        "#                 trunk.append(B)\n",
        "#                 indim = list_of_out_dims[i]\n",
        "\n",
        "#         if flatten:\n",
        "#             avgpool = nn.AvgPool2d(7)\n",
        "#             trunk.append(avgpool)\n",
        "#             trunk.append(Flatten())\n",
        "#             self.final_feat_dim = indim\n",
        "#         else:\n",
        "#             self.final_feat_dim = [ indim, 7, 7]\n",
        "\n",
        "#         self.trunk = nn.Sequential(*trunk)\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = self.trunk(x)\n",
        "#         return out\n",
        "\n",
        "def Conv4():\n",
        "    print(\"ConvNet(4)\")\n",
        "    return ConvNet(4)\n",
        "\n",
        "# def Conv6():\n",
        "#     print(\"ConvNet(6)\")\n",
        "#     return ConvNet(6)\n",
        "\n",
        "# def Conv4NP():\n",
        "#     return ConvNetNopool(4)\n",
        "\n",
        "# def Conv6NP():\n",
        "#     return ConvNetNopool(6)\n",
        "\n",
        "  \n",
        "# def Conv4S():\n",
        "#     return ConvNetS(4)\n",
        "\n",
        "# def Conv4SNP():\n",
        "#     return ConvNetSNopool(4)\n",
        "\n",
        "# def ResNet10( flatten = True):\n",
        "#     return ResNet(SimpleBlock, [1,1,1,1],[64,128,256,512], flatten)\n",
        "\n",
        "# def ResNet18( flatten = True):\n",
        "#     return ResNet(SimpleBlock, [2,2,2,2],[64,128,256,512], flatten)\n",
        "\n",
        "# def ResNet34( flatten = True):\n",
        "#     return ResNet(SimpleBlock, [3,4,6,3],[64,128,256,512], flatten)\n",
        "\n",
        "# def ResNet50( flatten = True):\n",
        "#     return ResNet(BottleneckBlock, [3,4,6,3], [256,512,1024,2048], flatten)\n",
        "\n",
        "# def ResNet101( flatten = True):\n",
        "#     return ResNet(BottleneckBlock, [3,4,23,3],[256,512,1024,2048], flatten)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0m0sAgI29Tz",
        "colab": {}
      },
      "source": [
        "class MetaTemplate(nn.Module):\n",
        "    def __init__(self, model_func, n_way, n_support, change_way = True):\n",
        "        super().__init__()\n",
        "        # print(\"__init__\")\n",
        "        self.n_way      = n_way\n",
        "        self.n_support  = n_support\n",
        "        self.n_query    = -1 #(change depends on input) \n",
        "        self.feature    = model_func()\n",
        "        self.feat_dim   = self.feature.final_feat_dim\n",
        "        self.change_way = change_way  #some methods allow different_way classification during training and test\n",
        "        \n",
        "\n",
        "    @abstractmethod\n",
        "    def set_forward(self,x,is_feature):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_forward_loss(self, x):\n",
        "        pass\n",
        "\n",
        "    def forward(self,x):\n",
        "        # print(\"forward\")\n",
        "        out  = self.feature.forward(x)\n",
        "        return out\n",
        "\n",
        "    def parse_feature(self,x,is_feature):\n",
        "        x    = V(x.cuda())\n",
        "        \n",
        "        print(\"parse_feature_x.size() {}\".format(x.size()))\n",
        "        \n",
        "        if is_feature:\n",
        "            z_all = x\n",
        "        else:\n",
        "            x           = x.contiguous().view( self.n_way * (self.n_support + self.n_query), *x.size()[2:]) \n",
        "            z_all       = self.feature.forward(x)\n",
        "            z_all       = z_all.view( self.n_way, self.n_support + self.n_query, -1)\n",
        "        z_support   = z_all[:, :self.n_support]\n",
        "        z_query     = z_all[:, self.n_support:]\n",
        "        \n",
        "#         global step__\n",
        "#         step__ += 1\n",
        "#         x_ = make_grid(z_support)\n",
        "#         writer.add_image('support_data', x_, step__)\n",
        "        \n",
        "#         y_ = make_grid(z_query)\n",
        "#         writer.add_image('query_data', y_, step__) \n",
        "\n",
        "        return z_support, z_query\n",
        "\n",
        "    def correct(self, x):       \n",
        "        scores = self.set_forward(x)\n",
        "        y_query = np.repeat(range( self.n_way ), self.n_query )\n",
        "        print(\"correct\")\n",
        "        topk_scores, topk_labels = scores.data.topk(1, 1, True, True)\n",
        "        topk_ind = topk_labels.cpu().numpy()\n",
        "        top1_correct = np.sum(topk_ind[:,0] == y_query)\n",
        "        return float(top1_correct), len(y_query)\n",
        "\n",
        "    def train_loop(self, epoch, train_loader, optimizer ):\n",
        "        print_freq = 10\n",
        "        global stepp\n",
        "        avg_loss=0\n",
        "        print(\"train_loop\")\n",
        "        for i, (x,_ ) in enumerate(train_loader):\n",
        "            self.n_query = x.size(1) - self.n_support           \n",
        "            if self.change_way:\n",
        "                self.n_way  = x.size(0)\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.set_forward_loss( x )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss = avg_loss+loss.item()\n",
        "\n",
        "            if i % print_freq==0:\n",
        "                #print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader), \n",
        "                                                                        avg_loss/float(i+1)))\n",
        "                stepp += 1\n",
        "                writer.add_scalar('loss_epoch', avg_loss/float(i+1), stepp)\n",
        "                \n",
        "\n",
        "    def test_loop(self, test_loader, record = None):\n",
        "        correct =0\n",
        "        count = 0\n",
        "        acc_all = []\n",
        "        print(\"test_loop\")\n",
        "        iter_num = len(test_loader) \n",
        "        for i, (x,_) in enumerate(test_loader):\n",
        "            self.n_query = x.size(1) - self.n_support\n",
        "            if self.change_way:\n",
        "                self.n_way  = x.size(0)\n",
        "            correct_this, count_this = self.correct(x)\n",
        "            acc_all.append(correct_this/ count_this*100  )\n",
        "\n",
        "        acc_all  = np.asarray(acc_all)\n",
        "        acc_mean = np.mean(acc_all)\n",
        "        acc_std  = np.std(acc_all)\n",
        "        \n",
        "        print('%d Test Acc = %4.2f%% +- %4.2f%%' %(iter_num,  acc_mean, 1.96* acc_std/np.sqrt(iter_num)))\n",
        "\n",
        "        return acc_mean\n",
        "\n",
        "    def set_forward_adaptation(self, x, is_feature = True): #further adaptation, default is fixing feature and train a new softmax clasifier\n",
        "        assert is_feature == True, 'Feature is fixed in further adaptation'\n",
        "        print(\"set_forward_adaptation\")\n",
        "        z_support, z_query  = self.parse_feature(x,is_feature)\n",
        "\n",
        "        z_support   = z_support.contiguous().view(self.n_way* self.n_support, -1 )\n",
        "        z_query     = z_query.contiguous().view(self.n_way* self.n_query, -1 )\n",
        "\n",
        "        y_support = torch.from_numpy(np.repeat(range( self.n_way ), self.n_support ))\n",
        "        y_support = V(y_support.cuda())\n",
        "\n",
        "        linear_clf = Linear(self.feat_dim, self.n_way)\n",
        "        linear_clf = linear_clf.cuda()\n",
        "\n",
        "        set_optimizer = SGD(linear_clf.parameters(), lr = 0.01, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
        "\n",
        "        loss_function = CrossEntropyLoss()\n",
        "        loss_function = loss_function.cuda()\n",
        "        \n",
        "        batch_size = 4\n",
        "        support_size = self.n_way* self.n_support\n",
        "        for epoch in range(100):\n",
        "            rand_id = np.random.permutation(support_size)\n",
        "            for i in range(0, support_size , batch_size):\n",
        "                set_optimizer.zero_grad()\n",
        "                selected_id = torch.from_numpy( rand_id[i: min(i+batch_size, support_size) ]).cuda()\n",
        "                z_batch = z_support[selected_id]\n",
        "                y_batch = y_support[selected_id] \n",
        "                scores = linear_clf(z_batch)\n",
        "                loss = loss_function(scores,y_batch)\n",
        "                loss.backward()\n",
        "\n",
        "        scores = linear_clf(z_query)\n",
        "        return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT8g9tWYTC4e",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "# Copyright 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "transformtypedict=dict(Brightness=ImageEnhance.Brightness, Contrast=ImageEnhance.Contrast, Sharpness=ImageEnhance.Sharpness, Color=ImageEnhance.Color)\n",
        "\n",
        "\n",
        "\n",
        "class ImageJitter(object):\n",
        "    def __init__(self, transformdict):\n",
        "        self.transforms = [(transformtypedict[k], transformdict[k]) for k in transformdict]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        out = img\n",
        "        randtensor = torch.rand(len(self.transforms))\n",
        "\n",
        "        for i, (transformer, alpha) in enumerate(self.transforms):\n",
        "            r = alpha*(randtensor[i]*2.0 -1.0) + 1\n",
        "            out = transformer(out).enhance(r).convert('RGB')\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3mLJehkTUw7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# This code is modified from https://github.com/facebookresearch/low-shot-shrink-hallucinate\n",
        "\n",
        "class TransformLoader:\n",
        "    def __init__(self, image_size, \n",
        "                 normalize_param    = dict(mean= [0.485, 0.456, 0.406] , std=[0.229, 0.224, 0.225]),\n",
        "                 jitter_param       = dict(Brightness=0.4, Contrast=0.4, Color=0.4)):\n",
        "        self.image_size = image_size\n",
        "        self.normalize_param = normalize_param\n",
        "        self.jitter_param = jitter_param\n",
        "    \n",
        "    def parse_transform(self, transform_type):\n",
        "        if transform_type=='ImageJitter':\n",
        "            method = add_transforms.ImageJitter( self.jitter_param )\n",
        "            return method\n",
        "        method = getattr(transforms, transform_type)\n",
        "        if transform_type=='RandomSizedCrop':\n",
        "            return method(self.image_size) \n",
        "        elif transform_type=='CenterCrop':\n",
        "            return method(self.image_size) \n",
        "        elif transform_type=='Resize':\n",
        "            return method([int(self.image_size*1.15), int(self.image_size*1.15)])\n",
        "        elif transform_type=='Normalize':\n",
        "            return method(**self.normalize_param )\n",
        "        else:\n",
        "            return method()\n",
        "\n",
        "    def get_composed_transform(self, aug = False):\n",
        "        if aug:\n",
        "            transform_list = ['RandomSizedCrop', 'ImageJitter', 'RandomHorizontalFlip', 'ToTensor', 'Normalize']\n",
        "        else:\n",
        "            transform_list = ['Resize','CenterCrop', 'ToTensor', 'Normalize']\n",
        "\n",
        "        transform_funcs = [ self.parse_transform(x) for x in transform_list]\n",
        "        transform = transforms.Compose(transform_funcs)\n",
        "        return transform\n",
        "\n",
        "class DataManager:\n",
        "    @abstractmethod\n",
        "    def get_data_loader(self, data_file, aug):\n",
        "        pass \n",
        "\n",
        "\n",
        "class SimpleDataManager(DataManager):\n",
        "    def __init__(self, image_size, batch_size):        \n",
        "        super(SimpleDataManager, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.trans_loader = TransformLoader(image_size)\n",
        "\n",
        "    def get_data_loader(self, data_file, aug): #parameters that would change on train/val set\n",
        "        transform = self.trans_loader.get_composed_transform(aug)\n",
        "        dataset = SimpleDataset(data_file, transform)\n",
        "        data_loader_params = dict(batch_size = self.batch_size, shuffle = True, num_workers = 12, pin_memory = True)       \n",
        "        data_loader = torch.utils.data.DataLoader(dataset, **data_loader_params)\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "class SetDataManager(DataManager):\n",
        "    def __init__(self, image_size, n_way, n_support, n_query, n_eposide =100):        \n",
        "        super(SetDataManager, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.n_way = n_way\n",
        "        self.batch_size = n_support + n_query\n",
        "        self.n_eposide = n_eposide\n",
        "\n",
        "        self.trans_loader = TransformLoader(image_size)\n",
        "\n",
        "    def get_data_loader(self, data_file, aug): #parameters that would change on train/val set\n",
        "        transform = self.trans_loader.get_composed_transform(aug)\n",
        "        dataset = SetDataset( data_file , self.batch_size, transform )\n",
        "        sampler = EpisodicBatchSampler(len(dataset), self.n_way, self.n_eposide )  \n",
        "        data_loader_params = dict(batch_sampler = sampler,  num_workers = 12, pin_memory = True)       \n",
        "        data_loader = torch.utils.data.DataLoader(dataset, **data_loader_params)\n",
        "        return data_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN7UUtrzTZD0",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class SimpleDataset:\n",
        "    def __init__(self, data_file, transform, target_transform=identity):\n",
        "        with open(data_file, 'r') as f:\n",
        "            self.meta = json.load(f)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        image_path = os.path.join(self.meta['image_names'][i])\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        target = self.target_transform(self.meta['image_labels'][i])\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta['image_names'])\n",
        "\n",
        "\n",
        "class SetDataset:\n",
        "    def __init__(self, data_file, batch_size, transform):\n",
        "        with open(data_file, 'r') as f:\n",
        "            self.meta = json.load(f)\n",
        " \n",
        "        self.cl_list = np.unique(self.meta['image_labels']).tolist()\n",
        "\n",
        "        self.sub_meta = {}\n",
        "        for cl in self.cl_list:\n",
        "            self.sub_meta[cl] = []\n",
        "\n",
        "        for x,y in zip(self.meta['image_names'],self.meta['image_labels']):\n",
        "            self.sub_meta[y].append(x)\n",
        "\n",
        "        self.sub_dataloader = [] \n",
        "        sub_data_loader_params = dict(batch_size = batch_size,\n",
        "                                  shuffle = True,\n",
        "                                  num_workers = 0, #use main thread only or may receive multiple batches\n",
        "                                  pin_memory = False)        \n",
        "        for cl in self.cl_list:\n",
        "            sub_dataset = SubDataset(self.sub_meta[cl], cl, transform = transform )\n",
        "            self.sub_dataloader.append( torch.utils.data.DataLoader(sub_dataset, **sub_data_loader_params) )\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        return next(iter(self.sub_dataloader[i]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cl_list)\n",
        "\n",
        "class SubDataset:\n",
        "    def __init__(self, sub_meta, cl, transform=transforms.ToTensor(), target_transform=identity):\n",
        "        self.sub_meta = sub_meta\n",
        "        self.cl = cl \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        #print( '%d -%d' %(self.cl,i))\n",
        "        image_path = os.path.join( self.sub_meta[i])\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        target = self.target_transform(self.cl)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sub_meta)\n",
        "\n",
        "class EpisodicBatchSampler(object):\n",
        "    def __init__(self, n_classes, n_way, n_episodes):\n",
        "        self.n_classes = n_classes\n",
        "        self.n_way = n_way\n",
        "        self.n_episodes = n_episodes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_episodes\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.n_episodes):\n",
        "            yield torch.randperm(self.n_classes)[:self.n_way]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU6xx8qYTc9g",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class SimpleHDF5Dataset:\n",
        "    def __init__(self, file_handle = None):\n",
        "        if file_handle == None:\n",
        "            self.f = ''\n",
        "            self.all_feats_dset = []\n",
        "            self.all_labels = []\n",
        "            self.total = 0 \n",
        "        else:\n",
        "            self.f = file_handle\n",
        "            self.all_feats_dset = self.f['all_feats'][...]\n",
        "            self.all_labels = self.f['all_labels'][...]\n",
        "            self.total = self.f['count'][0]\n",
        "           # print('here')\n",
        "    def __getitem__(self, i):\n",
        "        return torch.Tensor(self.all_feats_dset[i,:]), int(self.all_labels[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total\n",
        "\n",
        "def init_loader(filename):\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        fileset = SimpleHDF5Dataset(f)\n",
        "\n",
        "    #labels = [ l for l  in fileset.all_labels if l != 0]\n",
        "    feats = fileset.all_feats_dset\n",
        "    labels = fileset.all_labels\n",
        "    while np.sum(feats[-1]) == 0:\n",
        "        feats  = np.delete(feats,-1,axis = 0)\n",
        "        labels = np.delete(labels,-1,axis = 0)\n",
        "        \n",
        "    class_list = np.unique(np.array(labels)).tolist() \n",
        "    inds = range(len(labels))\n",
        "\n",
        "    cl_data_file = {}\n",
        "    for cl in class_list:\n",
        "        cl_data_file[cl] = []\n",
        "    for ind in inds:\n",
        "        cl_data_file[labels[ind]].append( feats[ind])\n",
        "\n",
        "    return cl_data_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1eDoDniTmg_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmWQpOueT5Vu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "def one_hot(y, num_class):         \n",
        "    return torch.zeros((len(y), num_class)).scatter_(1, y.unsqueeze(1), 1)\n",
        "\n",
        "def DBindex(cl_data_file):\n",
        "    class_list = cl_data_file.keys()\n",
        "    cl_num= len(class_list)\n",
        "    cl_means = []\n",
        "    stds = []\n",
        "    DBs = []\n",
        "    for cl in class_list:\n",
        "        cl_means.append( np.mean(cl_data_file[cl], axis = 0) )\n",
        "        stds.append( np.sqrt(np.mean( np.sum(np.square( cl_data_file[cl] - cl_means[-1]), axis = 1))))\n",
        "\n",
        "    mu_i = np.tile( np.expand_dims( np.array(cl_means), axis = 0), (len(class_list),1,1) )\n",
        "    mu_j = np.transpose(mu_i,(1,0,2))\n",
        "    mdists = np.sqrt(np.sum(np.square(mu_i - mu_j), axis = 2))\n",
        "    \n",
        "    for i in range(cl_num):\n",
        "        DBs.append( np.max([ (stds[i]+ stds[j])/mdists[i,j]  for j in range(cl_num) if j != i ]) )\n",
        "    return np.mean(DBs)\n",
        "\n",
        "def sparsity(cl_data_file):\n",
        "    class_list = cl_data_file.keys()\n",
        "    cl_sparsity = []\n",
        "    for cl in class_list:\n",
        "        cl_sparsity.append(np.mean([np.sum(x!=0) for x in cl_data_file[cl] ])  ) \n",
        "\n",
        "    return np.mean(cl_sparsity) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sgZiBMUT_4d",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "model_dict = dict(\n",
        "    Conv4 = Conv4,\n",
        "#             Conv4S = Conv4S,\n",
        "#             Conv6 = Conv6,\n",
        "#             ResNet10 = ResNet10,\n",
        "#             ResNet18 = ResNet18,\n",
        "#             ResNet34 = ResNet34,\n",
        "#             ResNet50 = ResNet50,\n",
        "#             ResNet101 = ResNet101\n",
        "    )\n",
        "\n",
        "def get_assigned_file(checkpoint_dir,num):\n",
        "    assign_file = os.path.join(checkpoint_dir, '{:d}.tar'.format(num))\n",
        "    return assign_file\n",
        "\n",
        "def get_resume_file(checkpoint_dir):\n",
        "    filelist = glob.glob(os.path.join(checkpoint_dir, '*.tar'))\n",
        "    if len(filelist) == 0:\n",
        "        return None\n",
        "\n",
        "    filelist =  [ x  for x in filelist if os.path.basename(x) != 'best_model.tar' ]\n",
        "    epochs = np.array([int(os.path.splitext(os.path.basename(x))[0]) for x in filelist])\n",
        "    max_epoch = np.max(epochs)\n",
        "    resume_file = os.path.join(checkpoint_dir, '{:d}.tar'.format(max_epoch))\n",
        "    return resume_file\n",
        "\n",
        "def get_best_file(checkpoint_dir):    \n",
        "    best_file = os.path.join(checkpoint_dir, 'best_model.tar')\n",
        "    if os.path.isfile(best_file):\n",
        "        return best_file\n",
        "    else:\n",
        "        return get_resume_file(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHUooGs7j0lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code is modified from https://github.com/dragen1860/MAML-Pytorch and https://github.com/katerakelly/pytorch-maml \n",
        "\n",
        "class MAML(MetaTemplate):\n",
        "    def __init__(self, model_func,  n_way, n_support, approx = False):\n",
        "        super(MAML, self).__init__( model_func,  n_way, n_support, change_way = False)\n",
        "\n",
        "        self.loss_fn = CrossEntropyLoss()\n",
        "        self.classifier = Linear_fw(self.feat_dim, n_way)\n",
        "        self.classifier.bias.data.fill_(0)\n",
        "        # self.conv2d_fw = Conv2d_fw(self.)\n",
        "        self.n_task     = 4\n",
        "        self.task_update_num = 5\n",
        "        self.train_lr = 0.01\n",
        "        self.approx = approx #first order approx.        \n",
        "        # print(\"__init__MAML\")\n",
        "\n",
        "    def forward(self,x):\n",
        "        print(\"forward_MAML_1\")\n",
        "        # print(\"MAML_forward_x.size() {}\".format(x.size()))\n",
        "        out  = self.feature.forward(x)\n",
        "        # print(\"MAML_forward_out.size() {}\".format(out.size()))\n",
        "        # writer.add_image('feature', out)\n",
        "        # print(\"forward_MAML_2\")\n",
        "        scores  = self.classifier.forward(out)\n",
        "        # print(\"MAML_forward_scores.size() {}\".format(scores.size()))\n",
        "        writer.add_embedding(scores, metadata=None, label_img=None, global_step=0, tag='default',\n",
        "                             metadata_header=None)\n",
        "        # writer.add_image('classifier', scores)\n",
        "        # print(\"forward_MAML_3\")\n",
        "        return scores\n",
        "\n",
        "    def set_forward(self,x, is_feature = False):\n",
        "        assert is_feature == False, 'MAML do not support fixed feature' \n",
        "        print(\"set_forward_MAML\")\n",
        "        x = x.cuda()\n",
        "        x_var = V(x)\n",
        "        \n",
        "        x_a_i = x_var[:,:self.n_support,:,:,:].contiguous().view( self.n_way* self.n_support, *x.size()[2:]) #support data \n",
        "        x_b_i = x_var[:,self.n_support:,:,:,:].contiguous().view( self.n_way* self.n_query,   *x.size()[2:]) #query data\n",
        "        y_a_i = V( torch.from_numpy( np.repeat(range( self.n_way ), self.n_support ) )).cuda() #label for support data\n",
        "        # print(\"x_var[:,:self.n_support,:,:,:].size() {}\".format(x_var[:,:self.n_support,:,:,:].size()))\n",
        "        # print(\"x_var[:,self.n_support:,:,:,:].size() {}\".format(x_var[:,self.n_support:,:,:,:].size()))\n",
        "        # print(\"x_a_i.size() {}\".format(x_a_i.size()))\n",
        "        # print(\"x_b_i.size() {}\".format(x_b_i.size()))\n",
        "        global step__\n",
        "        step__ = 1\n",
        "        x_ = make_grid(x_a_i)\n",
        "        writer.add_image('support_data', x_, step__)\n",
        "        \n",
        "        y_ = make_grid(x_b_i)\n",
        "        writer.add_image('query_data', y_, step__)\n",
        "        \n",
        "\n",
        "        \n",
        "        fast_parameters = list(self.parameters()) #the first gradient calcuated in line 45 is based on original weight\n",
        "        for weight in self.parameters():\n",
        "            weight.fast = None\n",
        "        self.zero_grad()\n",
        "\n",
        "        for task_step in range(self.task_update_num):\n",
        "            # print(\"set_forward_MAML_1\")\n",
        "            scores = self.forward(x_a_i)\n",
        "            # print(\"set_forward_MAML_2\")\n",
        "            set_loss = self.loss_fn( scores, y_a_i)\n",
        "            # print(\"set_forward_MAML_3\")\n",
        "            grad = torch.autograd.grad(set_loss, fast_parameters, create_graph=True) #build full graph support gradient of gradient\n",
        "            # print(\"set_forward_MAML_4\")\n",
        "            if self.approx:\n",
        "                grad = [ g.detach()  for g in grad ] #do not calculate gradient of gradient if using first order approximation\n",
        "            # print(\"set_forward_MAML_5\")\n",
        "            fast_parameters = []\n",
        "            for k, weight in enumerate(self.parameters()):\n",
        "                #for usage of weight.fast, please see Linear_fw, Conv_fw in backbone.py \n",
        "                # print(\"set_forward_MAML_6\")\n",
        "                if weight.fast is None:\n",
        "                    weight.fast = weight - self.train_lr * grad[k] #create weight.fast \n",
        "                else:\n",
        "                    weight.fast = weight.fast - self.train_lr * grad[k] #create an updated weight.fast, note the '-' is not merely minus value, but to create a new weight.fast \n",
        "                fast_parameters.append(weight.fast) #gradients calculated in line 45 are based on newest fast weight, but the graph will retain the link to old weight.fasts\n",
        "        global step_\n",
        "        step_ += 1\n",
        "        writer.add_histogram('weight.fast', weight.fast, step_)\n",
        "        # print(\"set_forward_MAML______\")\n",
        "        scores = self.forward(x_b_i)\n",
        "        return scores\n",
        "\n",
        "    def set_forward_adaptation(self,x, is_feature = False): #overwrite parrent function\n",
        "        print(\"set_forward_adaptation_MAML\")\n",
        "        raise ValueError('MAML performs further adapation simply by increasing task_upate_num')\n",
        "\n",
        "\n",
        "    def set_forward_loss(self, x):\n",
        "        print(\"set_forward_loss_MAML\")\n",
        "        scores = self.set_forward(x, is_feature = False)\n",
        "        y_b_i = V( torch.from_numpy( np.repeat(range( self.n_way ), self.n_query   ) )).cuda()\n",
        "        loss = self.loss_fn(scores, y_b_i)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_loop(self, epoch, train_loader, optimizer): #overwrite parrent function\n",
        "        print_freq = 10\n",
        "        avg_loss=0\n",
        "        task_count = 0\n",
        "        loss_all = []\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"train_loop_MAML\")\n",
        "        #train\n",
        "        for i, (x,_) in enumerate(train_loader):\n",
        "            self.n_query = x.size(1) - self.n_support\n",
        "            print(\"x.size() {}\".format(x.size()))\n",
        "            print(\"n_query {}\".format(self.n_query))\n",
        "            print(\"x.size(1) {}\".format(x.size(1)))\n",
        "            print(\"n_support {}\".format(self.n_support))\n",
        "            assert self.n_way  ==  x.size(0), \"MAML do not support way change\"\n",
        "\n",
        "            loss = self.set_forward_loss(x)\n",
        "            avg_loss = avg_loss+loss.item()\n",
        "            loss_all.append(loss)\n",
        "\n",
        "            task_count += 1\n",
        "            print(\"task_count {}\".format(task_count))\n",
        "\n",
        "            if task_count == self.n_task: #MAML update several tasks at one time\n",
        "                loss_q = torch.stack(loss_all).sum(0)\n",
        "                loss_q.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                task_count = 0\n",
        "                loss_all = []\n",
        "            optimizer.zero_grad()\n",
        "            if i % print_freq==0:\n",
        "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader), avg_loss/float(i+1)))\n",
        "                writer.add_scalar('loss_epoch_', avg_loss/float(i+1), epoch)\n",
        "                      \n",
        "    def test_loop(self, test_loader, return_std = False): #overwrite parrent function\n",
        "        correct =0\n",
        "        count = 0\n",
        "        acc_all = []\n",
        "        print(\"test_loop_MAML\")\n",
        "        iter_num = len(test_loader) \n",
        "        for i, (x,_) in enumerate(test_loader):\n",
        "            self.n_query = x.size(1) - self.n_support\n",
        "            assert self.n_way  ==  x.size(0), \"MAML do not support way change\"\n",
        "            correct_this, count_this = self.correct(x)\n",
        "            acc_all.append(correct_this/ count_this *100 )\n",
        "\n",
        "        acc_all  = np.asarray(acc_all)\n",
        "        acc_mean = np.mean(acc_all)\n",
        "        acc_std  = np.std(acc_all)\n",
        "        print('%d Test Acc = %4.2f%% +- %4.2f%%' %(iter_num,  acc_mean, 1.96* acc_std/np.sqrt(iter_num)))\n",
        "        if return_std:\n",
        "            return acc_mean, acc_std\n",
        "        else:\n",
        "            return acc_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaDYJTRbI3AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RelationNet(MetaTemplate):\n",
        "    def __init__(self, model_func,  n_way, n_support, loss_type = 'mse'):\n",
        "        super(RelationNet, self).__init__(model_func,  n_way, n_support)\n",
        "        \n",
        "        print(\"RelationNet__init__\")\n",
        "        self.loss_type = loss_type  #'softmax'# 'mse'\n",
        "        self.relation_module = RelationModule( self.feat_dim , 8, self.loss_type ) #relation net features are not pooled, so self.feat_dim is [dim, w, h] \n",
        "\n",
        "        if self.loss_type == 'mse':\n",
        "            self.loss_fn = MSELoss()  \n",
        "        else:\n",
        "            self.loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    def set_forward(self,x,is_feature = False):\n",
        "        print(\"RelationNet__set_forward_x.size() {}\".format(x.size()))\n",
        "        z_support, z_query  = self.parse_feature(x,is_feature)\n",
        "        print(\"RelationNet__set_forward__\")\n",
        "        z_support   = z_support.contiguous()\n",
        "        z_proto     = z_support.view( self.n_way, self.n_support, *self.feat_dim ).mean(1) \n",
        "        z_query     = z_query.contiguous().view( self.n_way* self.n_query, *self.feat_dim )\n",
        "\n",
        "        \n",
        "        z_proto_ext = z_proto.unsqueeze(0).repeat(self.n_query* self.n_way,1,1,1,1)\n",
        "        z_query_ext = z_query.unsqueeze(0).repeat( self.n_way,1,1,1,1)\n",
        "        z_query_ext = torch.transpose(z_query_ext,0,1)\n",
        "        extend_final_feat_dim = self.feat_dim.copy()\n",
        "        extend_final_feat_dim[0] *= 2\n",
        "        relation_pairs = torch.cat((z_proto_ext,z_query_ext),2).view(-1, *extend_final_feat_dim)\n",
        "        relations = self.relation_module(relation_pairs).view(-1, self.n_way)\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def set_forward_adaptation(self,x,is_feature = True): #overwrite parent function\n",
        "        assert is_feature == True, 'Finetune only support fixed feature' \n",
        "        print(\"RelationNet__set_forward_adaptation__\")\n",
        "        full_n_support = self.n_support\n",
        "        full_n_query = self.n_query\n",
        "        relation_module_clone = RelationModule( self.feat_dim , 8, self.loss_type )\n",
        "        relation_module_clone.load_state_dict(self.relation_module.state_dict())\n",
        "        z_support, z_query  = self.parse_feature(x,is_feature)\n",
        "        z_support   = z_support.contiguous()\n",
        "        set_optimizer = SGD(self.relation_module.parameters(), lr = 0.01, momentum=0.9, dampening=0.9, weight_decay=0.001)\n",
        "\n",
        "        self.n_support = 3\n",
        "        self.n_query = 2\n",
        "\n",
        "        z_support_cpu = z_support.data.cpu().numpy()\n",
        "        for epoch in range(100):\n",
        "            perm_id = np.random.permutation(full_n_support).tolist()            \n",
        "            sub_x = np.array([z_support_cpu[i,perm_id,:,:,:] for i in range(z_support.size(0))])\n",
        "            sub_x = torch.Tensor(sub_x).cuda()\n",
        "            if self.change_way:\n",
        "                self.n_way  = sub_x.size(0)\n",
        "            set_optimizer.zero_grad()\n",
        "            y = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
        "            scores = self.set_forward(sub_x, is_feature = True)\n",
        "            if self.loss_type == 'mse':\n",
        "                y_oh = one_hot(y, self.n_way)\n",
        "                y_oh = V(y_oh.cuda())            \n",
        "\n",
        "                loss =  self.loss_fn(scores, y_oh )\n",
        "            else:\n",
        "                y = V(y.cuda())\n",
        "                loss = self.loss_fn(scores, y )\n",
        "            loss.backward()\n",
        "            set_optimizer.step()\n",
        "\n",
        "        self.n_support = full_n_support\n",
        "        self.n_query = full_n_query\n",
        "        z_proto     = z_support.view( self.n_way, self.n_support, *self.feat_dim ).mean(1) \n",
        "        z_query     = z_query.contiguous().view( self.n_way* self.n_query, *self.feat_dim )\n",
        "\n",
        "        \n",
        "        z_proto_ext = z_proto.unsqueeze(0).repeat(self.n_query* self.n_way,1,1,1,1)\n",
        "        z_query_ext = z_query.unsqueeze(0).repeat( self.n_way,1,1,1,1)\n",
        "        z_query_ext = torch.transpose(z_query_ext,0,1)\n",
        "        extend_final_feat_dim = self.feat_dim.copy()\n",
        "        extend_final_feat_dim[0] *= 2\n",
        "        relation_pairs = torch.cat((z_proto_ext,z_query_ext),2).view(-1, *extend_final_feat_dim)\n",
        "        relations = self.relation_module(relation_pairs).view(-1, self.n_way)\n",
        "\n",
        "        self.relation_module.load_state_dict(relation_module_clone.state_dict())\n",
        "        return relations\n",
        "    def set_forward_loss(self, x):\n",
        "      \n",
        "        y = torch.from_numpy(np.repeat(range( self.n_way ), self.n_query ))\n",
        "        print(\"RelationNet__set_forward_loss__\")\n",
        "        scores = self.set_forward(x)\n",
        "        if self.loss_type == 'mse':\n",
        "            y_oh = one_hot(y, self.n_way)\n",
        "            y_oh = V(y_oh.cuda())            \n",
        "\n",
        "            return self.loss_fn(scores, y_oh )\n",
        "        else:\n",
        "            y = V(y.cuda())\n",
        "            return self.loss_fn(scores, y )\n",
        "\n",
        "class RelationConvBlock(Module):\n",
        "    def __init__(self, indim, outdim, padding = 0):\n",
        "        super(RelationConvBlock, self).__init__()\n",
        "        self.indim  = indim\n",
        "        self.outdim = outdim\n",
        "        self.C      = Conv2d(indim, outdim, 3, padding = padding )\n",
        "        self.BN     = BatchNorm2d(outdim, momentum=1, affine=True)\n",
        "        self.relu   = ReLU()\n",
        "        self.pool   = MaxPool2d(2)\n",
        "        print(\"RelationConvBlock__init__\")\n",
        "        self.parametrized_layers = [self.C, self.BN, self.relu, self.pool]\n",
        "\n",
        "        for layer in self.parametrized_layers:\n",
        "            init_layer(layer)\n",
        "\n",
        "        self.trunk = Sequential(*self.parametrized_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        print(\"RelationConvBlock__forward__x.size() {}\".format(x.size()))\n",
        "        out = self.trunk(x)\n",
        "        print(\"RelationConvBlock__forward__out.size() {}\".format(out.size()))\n",
        "        return out\n",
        "\n",
        "class RelationModule(Module):\n",
        "    \"\"\"docstring for RelationNetwork\"\"\"\n",
        "    def __init__(self,input_size,hidden_size, loss_type = 'mse'):        \n",
        "        super(RelationModule, self).__init__()\n",
        "        print(\"RelationModule__init__\")\n",
        "        self.loss_type = loss_type\n",
        "        padding = 1 if ( input_size[1] <10 ) and ( input_size[2] <10 ) else 0 # when using Resnet, conv map without avgpooling is 7x7, need padding in block to do pooling\n",
        "\n",
        "        self.layer1 = RelationConvBlock(input_size[0]*2, input_size[0], padding = padding )\n",
        "        self.layer2 = RelationConvBlock(input_size[0], input_size[0], padding = padding )\n",
        "\n",
        "        shrink_s = lambda s: int((int((s- 2 + 2*padding)/2)-2 + 2*padding)/2)\n",
        "\n",
        "        self.fc1 = Linear( input_size[0]* shrink_s(input_size[1]) * shrink_s(input_size[2]), hidden_size )\n",
        "        self.fc2 = Linear( hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        print(\"RelationModule__forward__x.size() {}\".format(x.size()))\n",
        "        out = self.layer1(x)\n",
        "        print(\"RelationModule__forward__out_layer1.size() {}\".format(out.size()))\n",
        "        out = self.layer2(out)\n",
        "        print(\"RelationModule__forward__out_layer2.size() {}\".format(out.size()))\n",
        "        out = out.view(out.size(0),-1)\n",
        "        print(\"RelationModule__forward__out_layer2_view.size() {}\".format(out.size()))\n",
        "        out = F.relu(self.fc1(out))\n",
        "        print(\"RelationModule__forward__out_fc1_relu.size() {}\".format(out.size()))\n",
        "        if self.loss_type == 'mse':\n",
        "            out = F.sigmoid(self.fc2(out))\n",
        "            print(\"RelationModule__forward__out_fc2_sigmoid.size() {}\".format(out.size()))\n",
        "        elif self.loss_type == 'softmax':\n",
        "            out = self.fc2(out)\n",
        "            print(\"RelationModule__forward__out_fc2.size() {}\".format(out.size()))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuC3g5I5UDGp",
        "colab_type": "code",
        "outputId": "37c19648-7c8d-4dfa-c869-b63ad468e0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4400
        }
      },
      "source": [
        "def train(base_loader, val_loader, model, optimization, start_epoch, stop_epoch, params):    \n",
        "    if optimization == 'Adam':\n",
        "        optimizer = Adam(model.parameters())\n",
        "    else:\n",
        "       raise ValueError('Unknown optimization, please define by yourself')\n",
        "    writer.add_graph(model, (torch.rand(25, 3, 84, 84).to(device),))\n",
        "    # writer.add_graph()\n",
        "    max_acc = 0       \n",
        "    print(\"train!!!\")\n",
        "    for epoch in range(start_epoch,stop_epoch):\n",
        "        model.train()\n",
        "        print(\"train_\")\n",
        "        model.train_loop(epoch, base_loader,  optimizer ) #model are called by reference, no need to return \n",
        "        print(\"train__\")\n",
        "        model.eval()\n",
        "        print(\"train___\")\n",
        "        if not os.path.isdir(params.checkpoint_dir):\n",
        "            os.makedirs(params.checkpoint_dir)\n",
        "\n",
        "        acc = model.test_loop( val_loader)\n",
        "        if acc > max_acc : #for baseline and baseline++, we don't use validation here so we let acc = -1\n",
        "            print(\"best model! save...\")\n",
        "            max_acc = acc\n",
        "            outfile = os.path.join(params.checkpoint_dir, 'best_model.tar')\n",
        "            torch.save({'epoch':epoch, 'state':model.state_dict()}, outfile)\n",
        "\n",
        "        if (epoch % params.save_freq==0) or (epoch==stop_epoch-1):\n",
        "            outfile = os.path.join(params.checkpoint_dir, '{:d}.tar'.format(epoch))\n",
        "            torch.save({'epoch':epoch, 'state':model.state_dict()}, outfile)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "np.random.seed(10)\n",
        "script = 'train'\n",
        "\n",
        "parser = argparse.ArgumentParser(description= 'few-shot script %s' %(script))\n",
        "parser.add_argument('--dataset'     , default='CUB',        help='CUB/miniImagenet/cross/omniglot/cross_char')\n",
        "parser.add_argument('--model'       , default='Conv4',      help='model: Conv{4|6} / ResNet{10|18|34|50|101}') # 50 and 101 are not used in the paper\n",
        "parser.add_argument('--method'      , default='maml_approx',   help='baseline/baseline++/protonet/matchingnet/relationnet{_softmax}/maml{_approx}') #relationnet_softmax replace L2 norm with softmax to expedite training, maml_approx use first-order approximation in the gradient for efficiency\n",
        "parser.add_argument('--train_n_way' , default=5, type=int,  help='class num to classify for training') #baseline and baseline++ would ignore this parameter\n",
        "parser.add_argument('--test_n_way'  , default=5, type=int,  help='class num to classify for testing (validation) ') #baseline and baseline++ only use this parameter in finetuning\n",
        "parser.add_argument('--n_shot'      , default=5, type=int,  help='number of labeled data in each class, same as n_support') #baseline and baseline++ only use this parameter in finetuning\n",
        "parser.add_argument('--train_aug'   , action='store_true',  help='perform data augmentation or not during training ') #still required for save_features.py and test.py to find the model path correctly\n",
        "\n",
        "if script == 'train':\n",
        "    parser.add_argument('--num_classes' , default=200, type=int, help='total number of classes in softmax, only used in baseline') #make it larger than the maximum label value in base class\n",
        "    parser.add_argument('--save_freq'   , default=50, type=int, help='Save frequency')\n",
        "    parser.add_argument('--start_epoch' , default=0, type=int,help ='Starting epoch')\n",
        "    parser.add_argument('--stop_epoch'  , default=-1, type=int, help ='Stopping epoch') #for meta-learning methods, each epoch contains 100 episodes. The default epoch number is dataset dependent. See train.py\n",
        "    parser.add_argument('--resume'      , default=True, action='store_true', help='continue from previous trained model with largest epoch')\n",
        "    parser.add_argument('--warmup'      , action='store_true', help='continue from baseline, neglected if resume is true') #never used in the paper\n",
        "elif script == 'save_features':\n",
        "    parser.add_argument('--split'       , default='novel', help='base/val/novel') #default novel, but you can also test base/val class accuracy if you want \n",
        "    parser.add_argument('--save_iter', default=-1, type=int,help ='save feature from the model trained in x epoch, use the best model if x is -1')\n",
        "elif script == 'test':\n",
        "    parser.add_argument('--split'       , default='novel', help='base/val/novel') #default novel, but you can also test base/val class accuracy if you want \n",
        "    parser.add_argument('--save_iter', default=-1, type=int,help ='saved feature from the model trained in x epoch, use the best model if x is -1')\n",
        "    parser.add_argument('--adaptation'  , action='store_true', help='further adaptation in test time or not')\n",
        "else:\n",
        "   raise ValueError('Unknown script')\n",
        "\n",
        "params = parser.parse_args('')\n",
        "\n",
        "\n",
        "if params.dataset == 'cross':\n",
        "    base_file = data_dir['miniImagenet'] + 'all.json' \n",
        "    val_file   = data_dir['CUB'] + 'val.json' \n",
        "elif params.dataset == 'cross_char':\n",
        "    base_file = data_dir['omniglot'] + 'noLatin.json' \n",
        "    val_file   = data_dir['emnist'] + 'val.json' \n",
        "else:\n",
        "    base_file = data_dir[params.dataset] + 'base.json' \n",
        "    val_file   = data_dir[params.dataset] + 'val.json' \n",
        "\n",
        "if 'Conv' in params.model:\n",
        "    if params.dataset in ['omniglot', 'cross_char']:\n",
        "        image_size = 28\n",
        "    else:\n",
        "        image_size = 84\n",
        "else:\n",
        "    image_size = 224\n",
        "\n",
        "if params.dataset in ['omniglot', 'cross_char']:\n",
        "    assert params.model == 'Conv4' and not params.train_aug ,'omniglot only support Conv4 without augmentation'\n",
        "    params.model = 'Conv4S'\n",
        "\n",
        "optimization = 'Adam'\n",
        "\n",
        "if params.stop_epoch == -1: \n",
        "    if params.method in ['baseline', 'baseline++'] :\n",
        "        if params.dataset in ['omniglot', 'cross_char']:\n",
        "            params.stop_epoch = 5\n",
        "        elif params.dataset in ['CUB']:\n",
        "            params.stop_epoch = 200 # This is different as stated in the open-review paper. However, using 400 epoch in baseline actually lead to over-fitting\n",
        "        elif params.dataset in ['miniImagenet', 'cross']:\n",
        "            params.stop_epoch = 400\n",
        "        else:\n",
        "            params.stop_epoch = 400 #default\n",
        "    else: #meta-learning methods\n",
        "        if params.n_shot == 1:\n",
        "            params.stop_epoch = 600\n",
        "        elif params.n_shot == 5:\n",
        "            params.stop_epoch = 400\n",
        "        else:\n",
        "            params.stop_epoch = 600 #default\n",
        "\n",
        "\n",
        "if params.method in ['baseline', 'baseline++'] :\n",
        "    base_datamgr    = SimpleDataManager(image_size, batch_size = 16)\n",
        "    base_loader     = base_datamgr.get_data_loader( base_file , aug = params.train_aug )\n",
        "    val_datamgr     = SimpleDataManager(image_size, batch_size = 64)\n",
        "    val_loader      = val_datamgr.get_data_loader( val_file, aug = False)\n",
        "\n",
        "    if params.dataset == 'omniglot':\n",
        "        assert params.num_classes >= 4112, 'class number need to be larger than max label id in base class'\n",
        "    if params.dataset == 'cross_char':\n",
        "        assert params.num_classes >= 1597, 'class number need to be larger than max label id in base class'\n",
        "\n",
        "    if params.method == 'baseline':\n",
        "        model           = BaselineTrain( model_dict[params.model], params.num_classes)\n",
        "    elif params.method == 'baseline++':\n",
        "        model           = BaselineTrain( model_dict[params.model], params.num_classes, loss_type = 'dist')\n",
        "\n",
        "elif params.method in ['protonet','matchingnet','relationnet', 'relationnet_softmax', 'maml', 'maml_approx']:\n",
        "    n_query = max(1, int(16* params.test_n_way/params.train_n_way)) #if test_n_way is smaller than train_n_way, reduce n_query to keep batch size small\n",
        "\n",
        "    train_few_shot_params    = dict(n_way = params.train_n_way, n_support = params.n_shot) \n",
        "    base_datamgr            = SetDataManager(image_size, n_query = n_query,  **train_few_shot_params)\n",
        "    base_loader             = base_datamgr.get_data_loader( base_file , aug = params.train_aug )\n",
        "\n",
        "    test_few_shot_params     = dict(n_way = params.test_n_way, n_support = params.n_shot) \n",
        "    val_datamgr             = SetDataManager(image_size, n_query = n_query, **test_few_shot_params)\n",
        "    val_loader              = val_datamgr.get_data_loader( val_file, aug = False) \n",
        "    #a batch for SetDataManager: a [n_way, n_support + n_query, dim, w, h] tensor        \n",
        "    print(\"hello\")\n",
        "    if params.method == 'protonet':\n",
        "        model           = ProtoNet( model_dict[params.model], **train_few_shot_params )\n",
        "    elif params.method == 'matchingnet':\n",
        "        model           = MatchingNet( model_dict[params.model], **train_few_shot_params )\n",
        "    elif params.method in ['relationnet', 'relationnet_softmax']:\n",
        "        if params.model == 'Conv4': \n",
        "            feature_model = Conv4NP\n",
        "        elif params.model == 'Conv6': \n",
        "            feature_model = Conv6NP\n",
        "        elif params.model == 'Conv4S': \n",
        "            feature_model = Conv4SNP\n",
        "        else:\n",
        "            feature_model = lambda: model_dict[params.model]( flatten = False )\n",
        "        loss_type = 'mse' if params.method == 'relationnet' else 'softmax'\n",
        "\n",
        "        model           = RelationNet( feature_model, loss_type = loss_type , **train_few_shot_params )\n",
        "    elif params.method in ['maml' , 'maml_approx']:\n",
        "        ConvBlock.maml = True\n",
        "        # SimpleBlock.maml = True\n",
        "        # BottleneckBlock.maml = True\n",
        "        # ResNet.maml = True\n",
        "        model           = MAML(  model_dict[params.model], approx = (params.method == 'maml_approx') , **train_few_shot_params )\n",
        "        if params.dataset in ['omniglot', 'cross_char']: #maml use different parameter in omniglot\n",
        "            model.n_task     = 32\n",
        "            model.task_update_num = 1\n",
        "            model.train_lr = 0.1\n",
        "else:\n",
        "   raise ValueError('Unknown method')\n",
        "\n",
        "print(\"hello!\")\n",
        "model = model.cuda()\n",
        "\n",
        "params.checkpoint_dir = '%s/checkpoints/%s/%s_%s' %(save_dir, params.dataset, params.model, params.method)\n",
        "if params.train_aug:\n",
        "    params.checkpoint_dir += '_aug'\n",
        "if not params.method  in ['baseline', 'baseline++']: \n",
        "    params.checkpoint_dir += '_%dway_%dshot' %( params.train_n_way, params.n_shot)\n",
        "\n",
        "if not os.path.isdir(params.checkpoint_dir):\n",
        "    os.makedirs(params.checkpoint_dir)\n",
        "\n",
        "start_epoch = params.start_epoch\n",
        "stop_epoch = params.stop_epoch\n",
        "if params.method == 'maml' or params.method == 'maml_approx' :\n",
        "    stop_epoch = params.stop_epoch * model.n_task #maml use multiple tasks in one update \n",
        "\n",
        "if params.resume:\n",
        "    resume_file = get_resume_file(params.checkpoint_dir)\n",
        "    if resume_file is not None:\n",
        "        tmp = torch.load(resume_file)\n",
        "        start_epoch = tmp['epoch']+1\n",
        "        model.load_state_dict(tmp['state'])\n",
        "elif params.warmup: #We also support warmup from pretrained baseline feature, but we never used in our paper\n",
        "    baseline_checkpoint_dir = '%s/checkpoints/%s/%s_%s' %(save_dir, params.dataset, params.model, 'baseline')\n",
        "    if params.train_aug:\n",
        "        baseline_checkpoint_dir += '_aug'\n",
        "    warmup_resume_file = get_resume_file(baseline_checkpoint_dir)\n",
        "    tmp = torch.load(warmup_resume_file)\n",
        "    if tmp is not None: \n",
        "        state = tmp['state']\n",
        "        state_keys = list(state.keys())\n",
        "        for i, key in enumerate(state_keys):\n",
        "            if \"feature.\" in key:\n",
        "                newkey = key.replace(\"feature.\",\"\")  # an architecture model has attribute 'feature', load architecture feature to backbone by casting name from 'feature.trunk.xx' to 'trunk.xx'  \n",
        "                state[newkey] = state.pop(key)\n",
        "            else:\n",
        "                state.pop(key)\n",
        "        model.feature.load_state_dict(state)\n",
        "    else:\n",
        "        raise ValueError('No warm_up file')\n",
        "\n",
        "model = train(base_loader, val_loader,  model, optimization, start_epoch, stop_epoch, params)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "ConvNet(4)\n",
            "__init__Linear_fw\n",
            "hello!\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_1\n",
            "x.size() torch.Size([25, 1600])\n",
            "out.size() torch.Size([25, 5])\n",
            "forward_Linear_fw_2\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train!!!\n",
            "train_\n",
            "train_loop_MAML\n",
            "x.size() torch.Size([5, 21, 3, 84, 84])\n",
            "n_query 16\n",
            "x.size(1) 21\n",
            "n_support 5\n",
            "set_forward_loss_MAML\n",
            "set_forward_MAML\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_1\n",
            "x.size() torch.Size([25, 1600])\n",
            "out.size() torch.Size([25, 5])\n",
            "forward_Linear_fw_2\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([80, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "task_count 1\n",
            "Epoch 1 | Batch 0/100 | Loss 1.487458\n",
            "x.size() torch.Size([5, 21, 3, 84, 84])\n",
            "n_query 16\n",
            "x.size(1) 21\n",
            "n_support 5\n",
            "set_forward_loss_MAML\n",
            "set_forward_MAML\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_1\n",
            "x.size() torch.Size([25, 1600])\n",
            "out.size() torch.Size([25, 5])\n",
            "forward_Linear_fw_2\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([80, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "task_count 2\n",
            "x.size() torch.Size([5, 21, 3, 84, 84])\n",
            "n_query 16\n",
            "x.size(1) 21\n",
            "n_support 5\n",
            "set_forward_loss_MAML\n",
            "set_forward_MAML\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_1\n",
            "x.size() torch.Size([25, 1600])\n",
            "out.size() torch.Size([25, 5])\n",
            "forward_Linear_fw_2\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([80, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "task_count 3\n",
            "x.size() torch.Size([5, 21, 3, 84, 84])\n",
            "n_query 16\n",
            "x.size(1) 21\n",
            "n_support 5\n",
            "set_forward_loss_MAML\n",
            "set_forward_MAML\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_1\n",
            "x.size() torch.Size([25, 1600])\n",
            "out.size() torch.Size([25, 5])\n",
            "forward_Linear_fw_2\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([25, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 42, 42])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 21, 21])\n",
            "Conv2d_fw_x.size() torch.Size([25, 64, 10, 10])\n",
            "forward_Linear_fw_fast\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "forward_MAML_1\n",
            "Conv2d_fw_x.size() torch.Size([80, 3, 84, 84])\n",
            "Conv2d_fw_x.size() torch.Size([80, 64, 42, 42])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-a0fd2ad8844a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No warm_up file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-180-a0fd2ad8844a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(base_loader, val_loader, model, optimization, start_epoch, stop_epoch, params)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moptimizer\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m#model are called by reference, no need to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-0b0e92ba2809>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, epoch, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_way\u001b[0m  \u001b[0;34m==\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MAML do not support way change\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_forward_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mloss_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-0b0e92ba2809>\u001b[0m in \u001b[0;36mset_forward_loss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_forward_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"set_forward_loss_MAML\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0my_b_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_way\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_query\u001b[0m   \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-0b0e92ba2809>\u001b[0m in \u001b[0;36mset_forward\u001b[0;34m(self, x, is_feature)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight.fast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# print(\"set_forward_MAML______\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_b_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-0b0e92ba2809>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"forward_MAML_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print(\"MAML_forward_x.size() {}\".format(x.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# print(\"MAML_forward_out.size() {}\".format(out.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# writer.add_image('feature', out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-170-3829e6e99fdc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;31m# print('out size {}'.format(out.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-170-3829e6e99fdc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;31m#         grid = make_grid(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m#         writer.add_image('ConvBlock', grid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-170-3829e6e99fdc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# print(\"grid size {}\".format(grid___.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conv2d_fw_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid___\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataformats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_image\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         self._get_file_writer().add_summary(\n\u001b[0;32m--> 384\u001b[0;31m             image(tag, img_tensor, dataformats=dataformats), global_step, walltime)\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataformats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(tensor, rescale, rois)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PNG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0mimage_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1929\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, check)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     ImageFile._save(im, _idat(fp, chunk),\n\u001b[0;32m--> 797\u001b[0;31m                     [(\"zip\", (0, 0)+im.size, 0, rawmode)])\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5EekloXZovb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpCNRC5ZouU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_fn = xm.get_log_fn()\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  xla_model.train(train_loader, optimizer, batch_size, log_interval=log_interval, metrics_debug=False,\n",
        "                  log_fn=log_fn)\n",
        "  accuracy = xla_model.test(test_loader, xm.category_eval_fn(F.nll_loss), batch_size, log_fn=log_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2Uc9nsUJ6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_evaluation(cl_data_file, model, n_way = 5, n_support = 5, n_query = 15, adaptation = False):\n",
        "    class_list = cl_data_file.keys()\n",
        "\n",
        "    select_class = random.sample(class_list,n_way)\n",
        "    z_all  = []\n",
        "    for cl in select_class:\n",
        "        img_feat = cl_data_file[cl]\n",
        "        perm_ids = np.random.permutation(len(img_feat)).tolist()\n",
        "        z_all.append( [ np.squeeze( img_feat[perm_ids[i]]) for i in range(n_support+n_query) ] )     # stack each batch\n",
        "\n",
        "    z_all = torch.from_numpy(np.array(z_all) )\n",
        "    model.n_query = n_query\n",
        "    if adaptation:\n",
        "        scores  = model.set_forward_adaptation(z_all, is_feature = True)\n",
        "    else:\n",
        "        scores  = model.set_forward(z_all, is_feature = True)\n",
        "    pred = scores.data.cpu().numpy().argmax(axis = 1)\n",
        "    y = np.repeat(range( n_way ), n_query )\n",
        "    acc = np.mean(pred == y)*100 \n",
        "    return acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    params = parse_args('test')\n",
        "\n",
        "    acc_all = []\n",
        "\n",
        "    iter_num = 600\n",
        "\n",
        "    few_shot_params = dict(n_way = params.test_n_way , n_support = params.n_shot) \n",
        "\n",
        "    if params.dataset in ['omniglot', 'cross_char']:\n",
        "        assert params.model == 'Conv4' and not params.train_aug ,'omniglot only support Conv4 without augmentation'\n",
        "        params.model = 'Conv4S'\n",
        "\n",
        "    if params.method == 'baseline':\n",
        "        model           = BaselineFinetune( model_dict[params.model], **few_shot_params )\n",
        "    elif params.method == 'baseline++':\n",
        "        model           = BaselineFinetune( model_dict[params.model], loss_type = 'dist', **few_shot_params )\n",
        "    elif params.method == 'protonet':\n",
        "        model           = ProtoNet( model_dict[params.model], **few_shot_params )\n",
        "    elif params.method == 'matchingnet':\n",
        "        model           = MatchingNet( model_dict[params.model], **few_shot_params )\n",
        "    elif params.method in ['relationnet', 'relationnet_softmax']:\n",
        "        if params.model == 'Conv4': \n",
        "            feature_model = Conv4NP\n",
        "        elif params.model == 'Conv6': \n",
        "            feature_model = Conv6NP\n",
        "        elif params.model == 'Conv4S': \n",
        "            feature_model = Conv4SNP\n",
        "        else:\n",
        "            feature_model = lambda: model_dict[params.model]( flatten = False )\n",
        "        loss_type = 'mse' if params.method == 'relationnet' else 'softmax'\n",
        "        model           = RelationNet( feature_model, loss_type = loss_type , **few_shot_params )\n",
        "    elif params.method in ['maml' , 'maml_approx']:\n",
        "        ConvBlock.maml = True\n",
        "        SimpleBlock.maml = True\n",
        "        BottleneckBlock.maml = True\n",
        "        ResNet.maml = True\n",
        "        model = MAML(  model_dict[params.model], approx = (params.method == 'maml_approx') , **few_shot_params )\n",
        "        if params.dataset in ['omniglot', 'cross_char']: #maml use different parameter in omniglot\n",
        "            model.n_task     = 32\n",
        "            model.task_update_num = 1\n",
        "            model.train_lr = 0.1\n",
        "    else:\n",
        "       raise ValueError('Unknown method')\n",
        "\n",
        "    model = model.cuda()\n",
        "\n",
        "    checkpoint_dir = '%s/checkpoints/%s/%s_%s' %(save_dir, params.dataset, params.model, params.method)\n",
        "    if params.train_aug:\n",
        "        checkpoint_dir += '_aug'\n",
        "    if not params.method in ['baseline', 'baseline++'] :\n",
        "        checkpoint_dir += '_%dway_%dshot' %( params.train_n_way, params.n_shot)\n",
        "\n",
        "    #modelfile   = get_resume_file(checkpoint_dir)\n",
        "\n",
        "    if not params.method in ['baseline', 'baseline++'] : \n",
        "        if params.save_iter != -1:\n",
        "            modelfile   = get_assigned_file(checkpoint_dir,params.save_iter)\n",
        "        else:\n",
        "            modelfile   = get_best_file(checkpoint_dir)\n",
        "        if modelfile is not None:\n",
        "            tmp = torch.load(modelfile)\n",
        "            model.load_state_dict(tmp['state'])\n",
        "\n",
        "    split = params.split\n",
        "    if params.save_iter != -1:\n",
        "        split_str = split + \"_\" +str(params.save_iter)\n",
        "    else:\n",
        "        split_str = split\n",
        "    if params.method in ['maml', 'maml_approx']: #maml do not support testing with feature\n",
        "        if 'Conv' in params.model:\n",
        "            if params.dataset in ['omniglot', 'cross_char']:\n",
        "                image_size = 28\n",
        "            else:\n",
        "                image_size = 84 \n",
        "        else:\n",
        "            image_size = 224\n",
        "\n",
        "        datamgr         = SetDataManager(image_size, n_eposide = iter_num, n_query = 15 , **few_shot_params)\n",
        "        \n",
        "        if params.dataset == 'cross':\n",
        "            if split == 'base':\n",
        "                loadfile = data_dir['miniImagenet'] + 'all.json' \n",
        "            else:\n",
        "                loadfile   = data_dir['CUB'] + split +'.json'\n",
        "        elif params.dataset == 'cross_char':\n",
        "            if split == 'base':\n",
        "                loadfile = data_dir['omniglot'] + 'noLatin.json' \n",
        "            else:\n",
        "                loadfile  = data_dir['emnist'] + split +'.json' \n",
        "        else: \n",
        "            loadfile    = data_dir[params.dataset] + split + '.json'\n",
        "\n",
        "        novel_loader     = datamgr.get_data_loader( loadfile, aug = False)\n",
        "        if params.adaptation:\n",
        "            model.task_update_num = 100 #We perform adaptation on MAML simply by updating more times.\n",
        "        model.eval()\n",
        "        acc_mean, acc_std = model.test_loop( novel_loader, return_std = True)\n",
        "\n",
        "    else:\n",
        "        novel_file = os.path.join( checkpoint_dir.replace(\"checkpoints\",\"features\"), split_str +\".hdf5\") #defaut split = novel, but you can also test base or val classes\n",
        "        cl_data_file = feat_loader.init_loader(novel_file)\n",
        "\n",
        "        for i in range(iter_num):\n",
        "            acc = feature_evaluation(cl_data_file, model, n_query = 15, adaptation = params.adaptation, **few_shot_params)\n",
        "            acc_all.append(acc)\n",
        "\n",
        "        acc_all  = np.asarray(acc_all)\n",
        "        acc_mean = np.mean(acc_all)\n",
        "        acc_std  = np.std(acc_all)\n",
        "        print('%d Test Acc = %4.2f%% +- %4.2f%%' %(iter_num, acc_mean, 1.96* acc_std/np.sqrt(iter_num)))\n",
        "    with open('./record/results.txt' , 'a') as f:\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime()) \n",
        "        aug_str = '-aug' if params.train_aug else ''\n",
        "        aug_str += '-adapted' if params.adaptation else ''\n",
        "        if params.method in ['baseline', 'baseline++'] :\n",
        "            exp_setting = '%s-%s-%s-%s%s %sshot %sway_test' %(params.dataset, split_str, params.model, params.method, aug_str, params.n_shot, params.test_n_way )\n",
        "        else:\n",
        "            exp_setting = '%s-%s-%s-%s%s %sshot %sway_train %sway_test' %(params.dataset, split_str, params.model, params.method, aug_str , params.n_shot , params.train_n_way, params.test_n_way )\n",
        "        acc_str = '%d Test Acc = %4.2f%% +- %4.2f%%' %(iter_num, acc_mean, 1.96* acc_std/np.sqrt(iter_num))\n",
        "        f.write( 'Time: %s, Setting: %s, Acc: %s \\n' %(timestamp,exp_setting,acc_str)  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVSYIvO8UNE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfSqK3QVUNeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh ./filelists/CUB/download_CUB.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j81gm8Dojj_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../gdrive/My\\ Drive/CUB_200_2011 ../gdrive/My\\ Drive/CUB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyc6qB0IXgjn",
        "colab_type": "code",
        "outputId": "24936d92-6ce0-4070-b3d8-6330970116e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "cwd = os.getcwd() \n",
        "data_path = './drive/My Drive/CUB/images/'\n",
        "savedir = './drive/My Drive/CUB/'\n",
        "dataset_list = ['base','val','novel']\n",
        "\n",
        "#if not os.path.exists(savedir):\n",
        "#    os.makedirs(savedir)\n",
        "\n",
        "folder_list = [f for f in listdir(data_path) if isdir(join(data_path, f))]\n",
        "folder_list.sort()\n",
        "label_dict = dict(zip(folder_list,range(0,len(folder_list))))\n",
        "\n",
        "classfile_list_all = []\n",
        "\n",
        "for i, folder in enumerate(folder_list):\n",
        "    folder_path = join(data_path, folder)\n",
        "    classfile_list_all.append( [ join(folder_path, cf) for cf in listdir(folder_path) if (isfile(join(folder_path,cf)) and cf[0] != '.')])\n",
        "    random.shuffle(classfile_list_all[i])\n",
        "\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    file_list = []\n",
        "    label_list = []\n",
        "    for i, classfile_list in enumerate(classfile_list_all):\n",
        "        if 'base' in dataset:\n",
        "            if (i%2 == 0):\n",
        "                file_list = file_list + classfile_list\n",
        "                label_list = label_list + np.repeat(i, len(classfile_list)).tolist()\n",
        "        if 'val' in dataset:\n",
        "            if (i%4 == 1):\n",
        "                file_list = file_list + classfile_list\n",
        "                label_list = label_list + np.repeat(i, len(classfile_list)).tolist()\n",
        "        if 'novel' in dataset:\n",
        "            if (i%4 == 3):\n",
        "                file_list = file_list + classfile_list\n",
        "                label_list = label_list + np.repeat(i, len(classfile_list)).tolist()\n",
        "\n",
        "    fo = open(savedir + dataset + \".json\", \"w\")\n",
        "    fo.write('{\"label_names\": [')\n",
        "    fo.writelines(['\"%s\",' % item  for item in folder_list])\n",
        "    fo.seek(0, os.SEEK_END) \n",
        "    fo.seek(fo.tell()-1, os.SEEK_SET)\n",
        "    fo.write('],')\n",
        "\n",
        "    fo.write('\"image_names\": [')\n",
        "    fo.writelines(['\"%s\",' % item  for item in file_list])\n",
        "    fo.seek(0, os.SEEK_END) \n",
        "    fo.seek(fo.tell()-1, os.SEEK_SET)\n",
        "    fo.write('],')\n",
        "\n",
        "    fo.write('\"image_labels\": [')\n",
        "    fo.writelines(['%d,' % item  for item in label_list])\n",
        "    fo.seek(0, os.SEEK_END) \n",
        "    fo.seek(fo.tell()-1, os.SEEK_SET)\n",
        "    fo.write(']}')\n",
        "\n",
        "    fo.close()\n",
        "    print(\"%s -OK\" %dataset)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base -OK\n",
            "val -OK\n",
            "novel -OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agadlu8mZZLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv novel.json ./filelists/CUB/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9qQe2BtYoXO",
        "colab_type": "code",
        "outputId": "929cbe95-7f95-4ad3-8b07-b66adf57b8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp -r ../gdrive/My\\ Drive/CUB_200_2011/ ./CUB_200_2011/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-iXF2vxYxSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ce8ID0afgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer.add_image?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp1JtrxiG1P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_grid?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNR0Ea4j3X1a",
        "colab_type": "code",
        "outputId": "0bd0ec27-4e48-4e22-96f0-e9bb5a25893e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `IPython.core` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_d_cUDmeFEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}